{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    import numpy\n",
    "    return (numpy.corrcoef(y_true, y_pred)[0,1])**2\n",
    "\n",
    "def date_2_year(date):\n",
    "    return(date.year)\n",
    "\n",
    "def datetojd(stddate): # Date to Julian day\n",
    "    sdtdate = stddate.timetuple()\n",
    "    jdate = sdtdate.tm_yday\n",
    "    return(jdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c317ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "# Define the working directory path here\n",
    "wd = \"/path/to/working/directory\"\n",
    "dataset = pd.read_csv(wd+\"Pusa_data.csv\")\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
    "dataset.insert(loc= 1, column= \"Year\", value= dataset['Date'].dt.year)\n",
    "dataset.insert(loc= 2, column= \"Jday\", value= dataset['Date'].apply(datetojd))\n",
    "\n",
    "# dropping the rows having NaN values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# To reset the indices\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "\n",
    "# Spliting Train and test set\n",
    "train = dataset[dataset[\"Year\"] < 2015] # Trainig set from 2010 to 2014\n",
    "test = dataset[dataset[\"Year\"] >= 2015] # Test set from 2015 to 2017\n",
    "\n",
    "X_train = train[['T_min','T_max','T_mean','Ra', 'Rs']].values\n",
    "X_test = test[['T_min','T_max','T_mean','Ra', 'Rs']].values\n",
    "y_train = train[['ETo']].values\n",
    "y_test = test[['ETo']].values\n",
    "\n",
    "# Reshape\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(\"Shape of input training set: {}\".format(X_train.shape))\n",
    "print(\"Shape of target training set: {}\".format(y_train.shape))\n",
    "print(\"Shape of input test set: {}\". format(X_test.shape))\n",
    "print(\"Shape of target test set: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6) (inputs)\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout) (x, x)\n",
    "    x = layers.Dropout (dropout)(x)\n",
    "    res = x + inputs\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\") (x)\n",
    "    x = layers.Dropout(dropout) (x)\n",
    "    x = layers.Conv1D(filters=inputs. shape[-1], kernel_size=1) (x)\n",
    "    return x + res\n",
    "    \n",
    "# Build the transformer model\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\") (x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\") (x)\n",
    "        x = layers.Dropout (mlp_dropout) (x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras. optimizers.Adam(learning_rate=1e-4))\n",
    "    return model\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# input_shape = X_train.shape[1:]\n",
    "\n",
    "reg = KerasRegressor(\n",
    "    model=build_model,\n",
    "    # optimizer__learning_rate=0.001, # Removed optimizer settings here, let GridSearchCV handle it if needed\n",
    "    model__input_shape=X_train.shape[1:],\n",
    "    model__head_size=32,\n",
    "    model__num_heads=4,\n",
    "    model__ff_dim=256,\n",
    "    model__num_transformer_blocks=4,\n",
    "    model__mlp_units=[128],\n",
    "    model__mlp_dropout=0.3,\n",
    "    model__dropout=0.25,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    random_state=123,\n",
    "    verbose=0 # Set verbose to 0 to reduce output during grid search\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'model__head_size': [32,64],\n",
    "    'model__num_heads': [4], # [4, 8]\n",
    "    'model__ff_dim': [256, 512, 1024], \n",
    "    'model__num_transformer_blocks': [4], # [2, 4]\n",
    "    'model__mlp_units': [[256], [512],], # [[64], [128], [256], [512], [128, 64], [256, 128]]\n",
    "    'model__mlp_dropout': [0.3],\n",
    "    'model__dropout': [0.25],\n",
    "    'optimizer': ['adam'],  # Specify the optimizer name\n",
    "    'optimizer__learning_rate': [1e-4], # Optimizer-specific parameter\n",
    "    'batch_size': [16], # [16, 32, 64, 128]\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "regressor = GridSearchCV(estimator=reg, param_grid=params, n_jobs=10, cv=5, verbose=1,)# scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "print(\"Starting GridSearchCV...\")\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# test result\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# train result\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "\n",
    "# Preparing Observed and Predicted Test dataset\n",
    "Test_res = pd.DataFrame({\"Observed\": y_test.ravel(),\n",
    "                         \"Predicted\": y_pred.ravel()})\n",
    "\n",
    "# Preparing Observed and Predicted Train dataset\n",
    "Train_res = pd.DataFrame({\"Observed\": y_train.ravel(),\n",
    "                          \"Predicted\": y_pred_train.ravel()})\n",
    "\n",
    "# Test Plot\n",
    "\n",
    "plt.scatter(Test_res.Observed, Test_res.Predicted, \n",
    "            c =\"red\", \n",
    "            linewidths = 0.5, \n",
    "            marker =\"o\", \n",
    "            edgecolor =\"black\", \n",
    "            s = 50)\n",
    "\n",
    "plt.xlim(left= np.mean(Test_res.values)-3*np.std(Test_res.values),\n",
    "         right= np.mean(Test_res.values)+3*np.std(Test_res.values))\n",
    "plt.ylim(bottom= np.mean(Test_res.values)-3*np.std(Test_res.values),\n",
    "         top= np.mean(Test_res.values)+3*np.std(Test_res.values))\n",
    "\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.text(np.mean(Test_res.values)-2.7*np.std(Test_res.values),\n",
    "         np.mean(Test_res.values)+2.4*np.std(Test_res.values),  \n",
    "         '$R^2$ = %0.3f' % R2(Test_res.Observed, Test_res.Predicted))\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_size_inches(4.5, 4.5)\n",
    "plt.show()\n",
    "plt.draw()\n",
    "\n",
    "# Train plot\n",
    "\n",
    "plt.scatter(Train_res.Observed, Train_res.Predicted, \n",
    "            c =\"blue\", \n",
    "            linewidths = 0.5, \n",
    "            marker =\"o\", \n",
    "            edgecolor =\"black\", \n",
    "            s = 50)\n",
    "\n",
    "plt.xlim(left= np.mean(Train_res.values)-3*np.std(Train_res.values),\n",
    "         right= np.mean(Train_res.values)+3*np.std(Train_res.values))\n",
    "plt.ylim(bottom= np.mean(Train_res.values)-3*np.std(Train_res.values),\n",
    "         top= np.mean(Train_res.values)+3*np.std(Train_res.values))\n",
    "\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.text(np.mean(Train_res.values)-2.7*np.std(Train_res.values),\n",
    "         np.mean(Train_res.values)+2.4*np.std(Train_res.values), \n",
    "         '$R^2$ = %0.3f' % R2(Train_res.Observed, Train_res.Predicted))\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_size_inches(4.5, 4.5)\n",
    "plt.show()\n",
    "plt.draw()\n",
    "\n",
    "# Export Test and Train dataset\n",
    "Test_res.to_csv(wd+\"Radiation_based_TNN_Test.csv\", index = False, header=True)\n",
    "Train_res.to_csv(wd+\"Radiation_based_TNN_Train.csv\", index = False, header=True)\n",
    "\n",
    "# Best parameter\n",
    "print(regressor.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
